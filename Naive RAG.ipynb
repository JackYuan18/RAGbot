{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e820c20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in /home/zyuan/anaconda3/lib/python3.12/site-packages (1.1.9)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy in /home/zyuan/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Collecting torch\n",
      "  Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /home/zyuan/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/zyuan/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in /home/zyuan/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/zyuan/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /home/zyuan/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /home/zyuan/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zyuan/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/zyuan/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/zyuan/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Using cached transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "Using cached sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Using cached faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "Using cached torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "Successfully installed faiss-cpu-1.12.0 huggingface-hub-0.34.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 sentence-transformers-5.1.0 tokenizers-0.22.0 torch-2.8.0 transformers-4.56.1\n",
      "Requirement already satisfied: PyPDF2 in /home/zyuan/anaconda3/lib/python3.12/site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet\n",
    "!pip install transformers sentence-transformers faiss-cpu numpy torch\n",
    "!pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ef5c49b-5764-43c8-90ca-103cdf2d6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a single PDF file.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \" \"\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_pdfs_from_directory(directory, chunk_size=1000):\n",
    "    \"\"\"Load text from all PDFs in a directory.\"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(directory, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if text:\n",
    "                # Split long text into smaller chunks (optional, for better retrieval)\n",
    "                chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]  # Split into ~1000 char chunks\n",
    "                documents.extend(chunks)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e80ec9be-5ef3-4497-9f7c-ed0a9663dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "pdf_directory = cwd+\"/Database\"\n",
    "\n",
    "documents = load_pdfs_from_directory(pdf_directory, chunk_size=500)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2105bc9-ecec-42ff-8603-94e18795df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "# documents = [\n",
    "#     \"The capital of France is Paris. It is known for the Eiffel Tower and rich cultural history.\",\n",
    "#     \"Python is a versatile programming language used for web development, data science, and AI.\",\n",
    "#     \"The theory of relativity was developed by Albert Einstein in the early 20th century.\",\n",
    "#     \"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data.\"\n",
    "# ]\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "retriever_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Encode the documents\n",
    "document_embeddings = retriever_model.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "# Create a FAISS index for similarity search\n",
    "dimension = document_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity\n",
    "index.add(document_embeddings)  # Add document embeddings to the index\n",
    "\n",
    "# Function to retrieve top-k relevant documents\n",
    "def retrieve_documents(query, k=5):\n",
    "    query_embedding = retriever_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [documents[idx] for idx in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf24bed-25cf-4a60-a04d-ab8415dbd65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f39f2f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(torch.cuda.is_available())  # Should print: True\n",
    "\n",
    "# Load a generative model\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     load_in_4bit=True,  # Quantization for GPU\n",
    "#     device_map=\"auto\"   # Auto-map to GPU/CPU\n",
    "# )\n",
    "# generator = pipeline('text-generation', model=model,tokenizer=tokenizer)\n",
    "\n",
    "generator = pipeline('text2text-generation', model='google/flan-t5-large', device=device)\n",
    "# Function to generate an answer\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    # Combine retrieved documents into a context\n",
    "    context = \" \".join(retrieved_docs)\n",
    "    prompt = f\"Question: {query}\\nContext: {context}\\nAnswer:\"\n",
    "    response = generator(prompt, max_length=500, num_return_sequences=1)\n",
    "    return response, response[0]['generated_text'].replace(\"Answer:\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ee47e-9246-429e-9f57-4d6f087d31c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80ac3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_application(query, k=2):\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_documents(query, k)\n",
    "    print(\"Retrieved Documents:\", retrieved_docs)\n",
    "    \n",
    "    # Step 2: Generate answer using retrieved documents\n",
    "    response, answer = generate_answer(query, retrieved_docs)\n",
    "    return response, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ec55ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: [' \\n• The applicable safety performance metrics.  \\n• The data that can be leveraged to define safety performance reference values.  \\nThis best practice outl ines a process for leveraging human driving data to establish safety performance targets for \\nADS-DV behaviors. The targets within the specific use -case , exemplified in this best practice , are based on \\nnaturalistic driving data from manually driven vehicles , in the hope of aiding understanding from a broad audience \\nof stakeholders.  \\nSev', 'tent with public expectations. People are already accustomed \\nto the risks associated with human -driven vehicles, and they expect  ADS-DVs to outperform average human drivers.  \\nThroughout the docume nt, we leverage an example use -case associated with a specific behavioral competency  \\nand analyze human drivers ’ behaviors through the usage of NDS. As mentioned previously , the outlined process  \\ncan be generalized and abstracted to remain applicable to othe r data sources and comparison point']\n",
      "Query: What does human driving data provide?\n",
      "Response: [{'generated_text': 'safety performance reference values'}]\n",
      "Answer: safety performance reference values\n"
     ]
    }
   ],
   "source": [
    "query = \"What does human driving data provide?\"\n",
    "response,answer = rag_application(query)\n",
    "print(\"Query:\", query)\n",
    "print(\"Response:\", response)\n",
    "print(\"Answer:\", answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf17963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8497fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
